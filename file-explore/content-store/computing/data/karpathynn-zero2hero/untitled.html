<p>YT Video: https://www.youtube.com/watch?v=VMj-3S1tku0</p>
<h4><code>micrograd</code> is an automatic gradient engine(back prop)</h4>
<p><em>Back Propagation</em> efficient evaluate the a gradient of a loss function with respect to the weight of a Neural Net (NN). Therefore, one can iteratively tune the weights to improve accuracy.</p>
<h4><code>micrograd</code> allows one to build mathematical expressions</h4>
<p>Aside, Rectified Linear Unit <code>.relu()</code>. Informally described as <em>keep or squash to zero</em>. Can otherwise be stated as, <code>relu = max(0, x)</code>, and with the latex expression.
<div class="math">\[\text{relu }= \begin{cases} 0 & \text{if } x < 0 \\ x & \text{if } x \ge 0 \end{cases}\]</div>

<strong>Jargon</strong>: &quot;Forward pass,&quot; meaning the series of expression calculated before the <code>.backward()</code> call (initialize back prop at node <code>g</code>)</p>
<div class="code-block"><span class="code-lang">python</span><button class="copy-btn" aria-label="Copy code">Copy</button><pre><code class="language-python">from micrograd.engine import Value

a = Value(-4.0)
b = Value(2.0)
c = a + b
d = a * b + b**3
c += c + 1
c += 1 + c + (-a)
d += d * 2 + (b + a).relu()
d += 3 * d + (b - a).relu()
e = c - d
f = e**2
g = f / 2.0
g += 10.0 / f
print(f'{g.data:.4f}') # prints 24.7041, the outcome of this forward pass
g.backward()
print(f'{a.grad:.4f}') # prints 138.8338, i.e. the numerical value of dg/da
print(f'{b.grad:.4f}') # prints 645.5773, i.e. the numerical value of dg/db
</code></pre></div>
<p>At node <code>g</code>, recursively, go through the expression graph to apply the chain rule from calculus. Every edge between nodes has a gradient, <code>a.grad</code>. <code>a.grad</code> how <code>a</code> is affecting <code>g</code>.
<img src="/graphics/hypothesis_node_graph_backprop.excalidraw|center" alt="hypothesis_node_graph_backprop" /></p>
<h4>Interpretation</h4>
<p><em>How does <code>g</code> respond when <code>a</code> is tweaked?</em>
If we make <code>a</code> slightly larger then because <code>138.8338</code> is positive <code>g</code> will grow and the slope of that growth is <code>138.8338</code>.</p>
<h2>Zooming Out</h2>
<p><code>micrograd</code> is a scalar value AutoGrad Engine. This is a pedagogical and in practice these scalars would n-dimensional tensors</p>
